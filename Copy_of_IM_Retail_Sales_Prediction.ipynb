{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Irfan270791/CAPSTONE-PROJECT-Regression--Retail-Sales-Prediction/blob/main/Copy_of_IM_Retail_Sales_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Retail Sales Prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -** Irfan Momin.\n",
        "##### **Team Member 2 -** Sushil Ghodwinde.\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project title \"Retail Sales Prediction\" focuses on forecasting the sales amount for Rossman Sotres up to six weeks in advance. Initially Exploratory Data Analysis (EDA) was conducted to gain insights into the dataset.Following the EDA, Data Wrangling techniques were applied to clean and preprocess the dataset.Additionally feature engineering techniques were employed to create new and meaningful features from the existing dataset.\n",
        "\n",
        "To predict the sales, various machine learning models were utilized, including Decision Tree Regression, Random Forest Regression, Gradient Boosting Regression & XGBoost Regression.\n",
        "\n",
        "The main objective of this project is to assess the effectiveness and performance of these different regression models in accurately predicting the sales amount."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Irfan270791/CAPSTONE-PROJECT-Regression--Retail-Sales-Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rossmann operates over 3000 drug stores in 7 European countries. Currently Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including  promotions, competition, school, and state holidays, seasonality and locality with thousands of individual managers prediciting sales based on their unique circumstances, the accuracy of results can be quite varied.\n",
        "\n",
        "We are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set.Note that some stores in the dataset were temporarily closed for refurbishment."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import tree\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, auc\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "\n",
        "# Set random seed value for reproducibility.\n",
        "np.random.seed(42)\n",
        "\n",
        "# Required library for hyperparameter Tuning.\n",
        "!pip install optuna --quiet\n",
        "\n",
        "# Required library for visualizing missing values.\n",
        "!pip install missingno --quiet\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import missingno as msno\n",
        "import matplotlib.gridspec as gridspec"
      ],
      "metadata": {
        "id": "XTqazti7UuZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3kJj6fi-V73N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Retail Sales Prediction/Rossmann Stores Data.csv')\n",
        "df_2 = pd.read_csv('/content/drive/MyDrive/Retail Sales Prediction/store (1).csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look - df & shape.\n",
        "print(df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look - df_2 & shape.\n",
        "print(df_2.shape)\n",
        "df_2.head()"
      ],
      "metadata": {
        "id": "4WV1VOaJX9vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have a fact table(df) that contains the sales data for each store & date and a dimension table(df_2) that contains each store information.\n",
        "* we can merge the fact table with the dimension table for easier analysis."
      ],
      "metadata": {
        "id": "z2UEF8CeYZSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge datasets.\n",
        "df_m = df.merge(df_2, on='Store', how='left')"
      ],
      "metadata": {
        "id": "AQBfJ40lZiDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data set first look.\n",
        "df_m.sample(5)"
      ],
      "metadata": {
        "id": "Fz6PCs0-aIsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f'The dataset has {df_m.shape[0]} rows and {df_m.shape[1]} columns')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df_m.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print('The dataset has:',df_m.duplicated().sum(),'duplicate rows')"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values = df_m.isna().sum().sort_values(ascending=False)\n",
        "print('Features with missing values',  'percent missing values:\\n')\n",
        "missing = missing_values[missing_values > 0]\n",
        "\n",
        "# Percentage of missing values.\n",
        "print((missing_values[missing_values > 0]* 100/df_m.shape[0]))"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "msno.matrix(df_m, figsize=(12, 4))"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset represents historical sales data for 1,115 Rossmann stores. The data contains 1,017,209 entries(rows) & 18 Features (Columns).The dataset contains no duplicate entries and 5 features with more than 30% missing values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df_m.columns.tolist()"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df_m.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset includes the following features:\n",
        "* **id** - Represents a unique identifier for a combination of Store & Date within the test set.\n",
        "* **Store** - A unique identifier for each store.\n",
        "* **Sales** - The turnover (sales)for given day, which is the target variable to be predicted.\n",
        "* **Customers** - The number of customers on a given day.\n",
        "* **Open** - An indicator of whether the store was open: 0=close, 1=open.\n",
        "* **StateHoliday** - Indicates a state holiday.Most store are closed on state holidays, except for a few exceptions.The values are: a=public holiday, b=Easter holiday, c=Christmas, 0=None.\n",
        "* **SchoolHoliday** - Indicates whether the (Store,Date) was affected by the clouser of public schools.\n",
        "* **Store Type** - Differentiates between four different store models: a,b,c,d.\n",
        "* **Assortment** - Describes the assortment level of the store: a=basic, b=extra, c=extended.\n",
        "* **CompetitionDistance** - The distance in meters to the nearest competitor store.\n",
        "* **CompetitionOpenSince[Month/Year]** - Provides an approximate year & month when the nearest competitior store was opened.\n",
        "* **Promo** - Indicates whether a store is running a promotion on a given day.\n",
        "* **Promo2** - Represents a continuing & consecutive promotion for some stores: 0=store is not participating, 1=store is participating.\n",
        "* **Promo2Since[Year/Week]** - Describes the year & calendar week when the store started participating in Promo2.\n",
        "* **PromoInterval** - Describes the consecutive intervals when Promo2 is started, specifying the months in which the promotion is started. For example, \"Feb,May,Aug,Nov\" means the promotions starts in February,May,August & Novemberof any given year for that Store."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df_m.nunique().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Dropping the duplicate rows if found.\n",
        "df_m.drop_duplicates(inplace=True)\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are data points when the stores are closed.\n",
        "# Checking the value counts during 'open' & 'closed'\n",
        "print(df_m['Open'].value_counts())"
      ],
      "metadata": {
        "id": "VV5Bos3bvkd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the sum of sales during 'open' & 'closed'\n",
        "print(df_m[['Open','Sales']].groupby(['Open']).sum())"
      ],
      "metadata": {
        "id": "AZt_5sJzwaEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Since there are no sales when the stores are 'closed', we can drop the values where store is 'closed'.\n",
        "* We can drop the 'Open' Columns since it contains only 1 unique values."
      ],
      "metadata": {
        "id": "WHQajtq7xO0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting only the rows with Open = 1.\n",
        "df_m2 = df_m[df_m['Open']== 1].drop('Open', axis=1).copy()\n",
        "\n",
        "# Converting Date to Datetime for analysis & feature engineering.\n",
        "df_m2['Date'] = pd.to_datetime(df_m2['Date'])\n",
        "\n",
        "# We can also convert object Dtype to category for reduced memory usage.\n",
        "for col in df_m2.select_dtypes('object').columns:\n",
        "  df_m2[col] = df_m2[col].astype('category')"
      ],
      "metadata": {
        "id": "ZU-MSKXNxpj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_m2.info()"
      ],
      "metadata": {
        "id": "JwZnLamYzX-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of unique values in each categorical columns.\n",
        "[df_m2[col].value_counts() for col in df_m2.select_dtypes('category').columns]"
      ],
      "metadata": {
        "id": "aOnXPhFCzwnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the unique values in 'StateHoliday'\n",
        "print(df_m2['StateHoliday'].unique())"
      ],
      "metadata": {
        "id": "Ek9HQY-30fBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The StateHoliday column contain a mix of integer & string representations of the \"0\" value. This can cause the value counts to show duplicates."
      ],
      "metadata": {
        "id": "uem3CuZp0zHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace all variations of \"0\" with single representation.\n",
        "df_m2['StateHoliday'] = df_m2['StateHoliday'].replace(['0', 0], '0')"
      ],
      "metadata": {
        "id": "dxWTHB9Z1Ntz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The dataset contains mix of categorical & numerical columns.\n",
        "* The sum of sales for 'closed' store are zero, Henece selecting only the rows with Open=1.\n",
        "* Dropped the 'Open' Column since it contains only one unique value(1 for open stores).\n",
        "* Converted the 'Date' column to datetime data type for analysis & feature engineering purposes.\n",
        "* The 'StateHoliday' column has mix of integer & string representations of the \"0\" value. Which has been resolved by replacing all variations of \"0\" with str(0).   "
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper functions for cleaning up the data(Helps better visualize the data)"
      ],
      "metadata": {
        "id": "Vi9KVzQH_RE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cap_outliers(df):\n",
        "  # Create a copy of the Dataframe to avoid modifying the original data.\n",
        "  cleaned_df = df.copy()\n",
        "\n",
        "  # Iterate over numerical columns.\n",
        "  for column in cleaned_df.select_dtypes(include=np.number):\n",
        "    # Calculate the 99th percentile value.\n",
        "    upper_cap_value = cleaned_df[column].quantile(0.99)\n",
        "    lower_cap_value = cleaned_df[column].quantile(0.01)\n",
        "\n",
        "    # Cap outliers to the 99th percentile value.\n",
        "    cleaned_df[column] = np.where(cleaned_df[column] > upper_cap_value, upper_cap_value, cleaned_df[column])\n",
        "    cleaned_df[column] = np.where(cleaned_df[column] < lower_cap_value, lower_cap_value, cleaned_df[column])\n",
        "\n",
        "  return cleaned_df"
      ],
      "metadata": {
        "id": "lPOqVDib_jP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1  Univariate Analysis."
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "df_temp = df_m2.copy()\n",
        "# Numerical features\n",
        "numerical_features = (df_temp.select_dtypes(include=['int64', 'float64']).columns)\n",
        "\n",
        "for feature in numerical_features:\n",
        "  plt.figure(figsize=(6, 4))\n",
        "  sns.histplot(df_temp[feature], kde=True, bins=30)\n",
        "  plt.title(f'Distribution of {feature}')\n",
        "  plt.show()\n",
        "\n",
        "  # Central tendencies and dispersion.\n",
        "  mean = df_temp[feature].mean()\n",
        "  median = df_temp[feature].median()\n",
        "  std_dev = df_temp[feature].std()\n",
        "  print(f'Mean of {feature}: {mean}')\n",
        "  print(f'Median of {feature}: {median}')\n",
        "  print(f'Standard deviation of {feature}: {std_dev}')\n",
        "  print(f'Min value of {feature}: {df_temp[feature].min()}')\n",
        "  print(f'Max value of {feature}: {df_temp[feature].max()}')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A Histogram allow us to see the distribution of data & Understand its central tendency & the spread of the dataset."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The sales data is available from 2013-01-01 to 2015-07-31 for a duration of 941 days.\n",
        "* Stores are closed during Sundays hence the sales are reported zero.\n",
        "* Only few data points are available during School holidays(SchoolHoliday=1) & StateHolidays(StateHoliday=(a,b,c)).\n",
        "* The Distribution of competition distance is right skewed & ranges from 0 to 70000.\n",
        "* The distribution of competition open since year is left skewed & ranges from 1900 to 2015.\n",
        "* The distribution of competition distance is right skewed & ranges from 20 to 75860."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Examining sales data during school & state holidays can help in devising targeted marketing campaigns to attract customers during this periods.\n",
        "* Understanding the distribution of competition distance can help in strategic placement of new stores to minimize competition & maximize market share."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 Histogram of sales."
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "data = df_m2\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Histogram of sales')\n",
        "sns.histplot(data['Sales'])\n",
        "\n",
        "# Calculating the mean & median.\n",
        "mean_value = data['Sales'].mean().round(2)\n",
        "median_value = data['Sales'].median().round(2)\n",
        "\n",
        "# Drawing the lines for mean & median.\n",
        "plt.axvline(mean_value, color='red', linestyle='--', label='Mean:'+ str(mean_value))\n",
        "plt.axvline(median_value, color= 'blue', linestyle='--', label='Median:'+str(median_value))\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Sales Spread')\n",
        "sns.boxplot(y= data['Sales'])\n",
        "plt.show()\n",
        "\n",
        "print('5th Percentile of sales', data['Sales'].quantile(0.05))\n",
        "print('95th Percentile of sales', data['Sales'].quantile(0.95))"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A Histogram allow us to see the distribution of data & understand its central tendency & speard of dataset.\n",
        "* A box plot provides a summary of the distribution of a dataset,including information about the median,quartiles & potential outliers."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The distribution of sales seems to be skewed to the right.\n",
        "* 90% of the time the sales per day are within the interval 3173 & 12668.\n",
        "* However the top 5% of the sales are within the interval 12668 & 41551"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Recognizing the skewness in the sales distribution allows businesses to tailor marketing strategies to address both the majority of sales & the occasional high sales events.this approch can improve marketing ROI & customer engagement.\n",
        "* Understanding the distribution of sales can helps business identify periods of high sales activity. This can be leveraged to optimize inventory level, marketing efforts during peak sales periods,leading to increased revenue."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 Categorical features VS Sales histogram/Box plot.\n",
        "categorical features (StateHoliday, StoreType, Assortment, PromoInterval)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style= \"whitegrid\")\n",
        "plt.figure(figsize=(10, 20))\n",
        "\n",
        "input = cap_outliers(df_m2.copy())\n",
        "columns = ['Promo', 'Promo2', 'SchoolHoliday', 'StateHoliday', 'StoreType', 'Assortment', 'PromoInterval']\n",
        "colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold']\n",
        "\n",
        "color_cycle = itertools.cycle(colors)\n",
        "\n",
        "for i, col in enumerate(columns):\n",
        "  color_name = next(color_cycle)\n",
        "  plt.subplot(7,2, 2*i+1)\n",
        "  plt1 = input[col].value_counts().plot(kind='bar', color = color_name)\n",
        "  plt.title(f'{col} Histogram')\n",
        "  plt1.set(xlabel=col, ylabel= 'Count of sales')\n",
        "\n",
        "  plt.subplot(7, 2, 2*i+2)\n",
        "  sns.boxplot(x=col, y='Sales', data=input, color=color_name)\n",
        "  plt.title(f'Sales over {col} Boxplot')\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Sales')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A Histogram allows us to see the distribution of data & understand its central tendency and the spread of the dataset.\n",
        "* A Boxplot provides a summary of the distribution of a dataset, including information about the median, quartiles & potential outliers."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Promotion(Promo):** - The median value of sales is higher when there is a promotion compared to when there is no promotion.\n",
        "2. **Promotion Type2 (Promo2):** - The count & median value of sales is relatively similar for both Promo2 Types.\n",
        "3. **School Holiday:** - The median sales are slightly higher during school holidays.\n",
        "4. **State Holiday:** - The median sales are highest during state holiday type 'b'.\n",
        "5. **Store type:** - The count of the sales is highest for store type 'a'\n",
        "                   -The median sales are highest for store type 'b'.\n",
        "6. **Assortment:** -The count of sales is highest for assortment type 'a'.\n",
        "                  - The median sales are highest for assortment type 'c'.\n",
        "7. **Promotion Interval(PromoInterval)**:   - The count of sales is highest for the promo interval 'jan,Apr,Jul,Oct'.\n",
        "- The median sales are highest for the promo interval 'Feb,May,Aug,Nov'.\n",
        "                                    "
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Understanding the impact of promotions,storetype,assortment and holiday periods on sales can help Rossmann optimize their strategies.\n",
        "* Targeting the promotion during high-sales periods, such as school holidays or specific state holidays, can lead to increased sales & better resource utilization."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - Median sales over year/Quarter/Month/Week."
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "# Creat a copy of dataframe.\n",
        "df_temp = df_m.copy()\n",
        "df_temp['Date'] = pd.to_datetime(df_temp['Date'])\n",
        "\n",
        "# Extract Year, Month, Week & Quarter from the Date.\n",
        "df_temp['year'] = df_temp['Date'].dt.year\n",
        "df_temp['month'] = df_temp['Date'].dt.month_name()  # Getting the months name.\n",
        "df_temp['week'] = df_temp['Date'].dt.day_name()   #Getting the weekdays name.\n",
        "df_temp['quarter'] = 'Q'+ df_temp['Date'].dt.quarter.astype(str)  # Getting the quarters.\n",
        "\n",
        "# Specifying the correct order for months & Weeks.\n",
        "month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "week_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "\n",
        "# Covert 'month' & 'week' columns to categorical type with specified order.\n",
        "df_temp['month'] = pd.Categorical(df_temp['month'], categories = month_order, ordered= True)\n",
        "df_temp['week']= pd.Categorical(df_temp['week'], categories= week_order, ordered= True)\n",
        "\n",
        "# Set the seaborn style & color palette.\n",
        "sns.set_style(\"whitegrid\")\n",
        "colors = itertools.cycle(['skyblue', 'lightgreen', 'lightcoral', 'gold'])\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "for i,col in enumerate(['year', 'quarter', 'month', 'week']):\n",
        "    # Group by year, quarter, month, week & find the median of sales.\n",
        "    sales_data = df_temp.groupby([col])['Sales'].median()\n",
        "\n",
        "    # Plotting\n",
        "    ax = axes[i // 2, i % 2]  # selecting the appropriate subplot.\n",
        "    sns.barplot(x=sales_data.index, y=sales_data.values, color=next(colors), edgecolor='black', ax=ax)\n",
        "\n",
        "    ax.set_title(f'Median Sales over {col.capitalize()}', fontsize=16)\n",
        "    ax.set_xlabel(f'{col.capitalize()}', fontsize=14)\n",
        "    ax.set_ylabel('Median Sales', fontsize=14)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)  # Rotate x-labels for better visibility.\n",
        "\n",
        "plt.tight_layout()  # Adjusting spacing between subplots.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot help us to compare a numerical variable across a category. Categorical variable is plotted along the horizontal and the height of the bar represent the value of the numerical variable."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Yearly Sales:\n",
        "   * The sales data available for the year 2013, 2014 & 2015.\n",
        "   * Sales increased from 5,599.0 in 2013 to 5,918.0 in 2015, showing a positive trend over the years.\n",
        "\n",
        "2. Quarterly Sales:\n",
        "   * The highest sales were observed in Q4 with a value of 6,044.0 suggesting that the holiday season might contribute significantly to sales.\n",
        "   * Q1 had the lowest sales with a value of 5,614.0.\n",
        "3. Monthly Sales:\n",
        "   * December recorded the highest sales with a value of 6,732.0 indicating a peak in sales during holiday season .\n",
        "   * January had the lowest sales with a value of 5,484.0 possibly due to reduced consumer spending after the holiday season.\n",
        "   * Sales remained relatively stable from February to November, ranging from 5,611.0 to 6,083.0 with minor fluctuations.\n",
        "4. Weekly Sales:\n",
        "   * The highest sales are observed on Monday with a value of 7,311.0 indicating strong sales at the beginning of the week.\n",
        "   * Sunday had no sales recorded, suggesting that the business might be closed on sundays.\n",
        "   * Sales remained relatively consistent throughout the week.\n",
        "   * Saturday had comparatively lower sales with a value of 5410.0         "
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The business experiences seasonal variation in sales,with the highest sales occurring in Q4 during the holiday season and the lowest sales in Q1.\n",
        "* Heavy reliance on the holiday season for generating significant revenue can pose challenges in maintaining consistent sales throughout the year. to mitigate negative growth during non-peak seasons,the business should explore strategies to stimulate demand & attract customers during off-peak periods."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5  Continuous features VS Sales - Scatterplot."
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Numerical Columns.\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#  Assuming you have your data stored in a DataFrame called 'df'\n",
        "# 'Sales' is the dependent variable & other numerical columns are independent variables.\n",
        "\n",
        "# numerical_columns = df_m2.select_dtypes(include=['float64','int64']).columns\n",
        "numerical_columns = ['CompetitionDistance', 'Customers']\n",
        "\n",
        "# Calculate the number of rows & columns needed for the subplots.\n",
        "num_rows = (len(numerical_columns) // 2) + (len(numerical_columns) % 2)\n",
        "num_cols = 2\n",
        "\n",
        "# Create subplot with 2 columns\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 6))\n",
        "\n",
        "# Flatten the axes array if necessary.\n",
        "axes = axes.flatten() if isinstance(axes, np.ndarray) else axes\n",
        "\n",
        "# Iterate through each numerical column & create scatter plots in the subplots.\n",
        "for i, column in enumerate(numerical_columns):\n",
        "    sns.scatterplot(x=column, y='Sales', data=df_m2, ax=axes[i], alpha=0.1, markers=['o'])\n",
        "    axes[i].set_title(f'Scatterplot for {column} VS Sales')\n",
        "\n",
        "# Remove any empty subplot if the number of variable is odd\n",
        "if len(numerical_columns) % 2 !=0:\n",
        "   axes[-1].remove()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Scatter plot help us to visualize the realtionship between two continuous variables.it consists of a grid where each point represents an observation, plotted along the horizontal & vertical axes corresponding to the two variables being analyzed."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Competition Distance VS sales**- The competition distance is negatively correlated with sales. suggesting that the area with higher competition generates higher sales.\n",
        "\n",
        "* **Customers VS sales**- The customers are positively correlated with sales. increased number of customers visiting the store leads to higher sales."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Strategic decisions regarding store locations can be made based on competition distance to optimize sales & capture a larger market share."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6  Multiclass variables VS Sales."
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "multiclass = ['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
        "sample_data = cap_outliers(df_m2.reset_index(drop=True).copy())\n",
        "target = 'Sales'\n",
        "\n",
        "fig,axes = plt.subplots(len(multiclass),2, figsize=(15, 15))\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "for i, feature in enumerate(multiclass):\n",
        "    sns.boxplot(x=feature, y=target, data=sample_data, ax=axes[i, 0])\n",
        "    axes[i, 0].set_title(f'Box plot: {feature} VS {target}')\n",
        "    axes[i, 0].set_xlabel(feature)\n",
        "    axes[i, 0].set_ylabel(target)\n",
        "    axes[i, 0].tick_params(rotation=45)\n",
        "\n",
        "    sns.lineplot(x=feature, y=target, data=sample_data, ax=axes[i, 1])\n",
        "    axes[i, 1].set_title(f'Line Plot: {feature} VS {target}')\n",
        "    axes[i, 1].set_xlabel(feature)\n",
        "    axes[i, 1].set_ylabel(target)\n",
        "    axes[i, 1].tick_params(rotation=45)\n",
        "\n",
        "    # Calculate correlation coefficient.\n",
        "    correlation = sample_data[feature].corr(sample_data[target])\n",
        "\n",
        "    # Set color based on correlation sign.\n",
        "    color = 'lightgreen' if correlation >= 0 else 'lightcoral'\n",
        "\n",
        "    # Add correlation value as  a legend with color.\n",
        "    axes[i, 1].legend([f'Correlation: {correlation:.2f}'], facecolor=color)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A line plot, is graphical representation of data points connected by stright lines.it is commonly used to show the relationship between two continuous variable or to display the change in variable over time."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Median Sales by CompetitionOpenSinceMonth:\n",
        "   * There seems to be some trend in which highest median sales are observed in june(6.0)\n",
        "   * The lowest median sales are observed in February(2.0)\n",
        "\n",
        "2. Median Sales by CompetitionOpenSinceYear:\n",
        "   * Median sales seems to vary across different years, indicating that the year of competition opening might have an impact on sales performance.\n",
        "\n",
        "3. Median Sales by Promo2SinceWeek:\n",
        "   * The varying median sales suggest that the timing of promo2 activation might influence the sales performance of stores.\n",
        "\n",
        "4. Median Sales by Promo2SinceYear:\n",
        "   * The median sales vary across different years of promo2 activation.\n",
        "   * Stores that activated Promo2 in 2014 have the highest medain sales.         "
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights on sales trends based on competiton opening, Promo2 activation, and timing can help business optimize strategies and potentially generate positive business impact."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 Sales Volume and Medain Sales by Month."
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "import pandas as pd\n",
        "\n",
        "# Create a copy of the dataframe.\n",
        "df_temp = df_m.copy()\n",
        "\n",
        "# Extract year and month from date.\n",
        "df_temp['year'] = pd.to_datetime(df_temp['Date']).dt.year\n",
        "df_temp['month'] = pd.to_datetime(df_temp['Date']).dt.month\n",
        "\n",
        "# Groupby year, month & find the median of sales.\n",
        "sales_data = df_temp.groupby(['year', 'month']).Sales.median()\n",
        "\n",
        "# Find sales volume.\n",
        "sales_volume = df_temp.groupby(['year', 'month']).size()\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(15, 6))\n",
        "\n",
        "# plot medain sales as a line plot.\n",
        "sales_data.plot(kind='line', ax=ax1, color='blue', legend=True)\n",
        "\n",
        "# Create another y-axis for sales volume.\n",
        "ax2  = ax1.twinx()\n",
        "sales_volume.plot(kind= 'bar', ax=ax2, alpha=0.3, legend=True)\n",
        "\n",
        "ax1.set_title('Median Sales and Sales Volume Over Month and Years')\n",
        "ax1.set_xlabel('Time (year, Month)')\n",
        "ax1.set_ylabel('Median Sales')\n",
        "ax2.set_ylabel('Sales Volume')\n",
        "\n",
        "# Make the new labels for x-axis where only the first occurrence of each year in labeled.\n",
        "labels = [(2013, 'jan'),('feb'),('mar'),('apr'),('may'),('jun'),('jul'),('aug'),('sep'),('oct'),('nov'),('dec'),\n",
        "          (2014, 'jan'),('feb'),('mar'),('apr'),('may'),('jun'),('jul'),('aug'),('sep'),('oct'),('nov'),('dec'),\n",
        "          (2015, 'jan'),('feb'),('mar'),('apr'),('may'),('jun'),('jul')]\n",
        "\n",
        "ax1.set_xticks(range(len(labels)))\n",
        "ax2.set_xticks(range(len(labels)))\n",
        "\n",
        "# Set labels for both axis after the plots are created.\n",
        "ax1.set_xticklabels(labels, rotation=20, ha='right')\n",
        "ax2.set_xticklabels(labels, rotation=20, ha='right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot help us to visualize the relatioship between two continuous variables or to display the change in variable over time."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Seasonal Sales Trend:** - There seems to be a noticeable seasonal pattern in sales, with peaks occurring in December & lower points during the early months of the year.This suggests a possible relationship between sales and the holiday season.\n",
        "\n",
        "* **Increasing Sales Volume**- The sales volume represented by the \"Sales_Volume\"data, shows a general increasing trend over time.this suggests that the number of sales transaction has been growing steadily.\n",
        "\n",
        "* **Sales Fluctuations** - While the sales volume exhibits a consistent upward trend, the actual sales figure show some fluctuations from month to month.These fluctuation may be influenced by various factors,such as promotions, external events or changes in customer behaviour.\n",
        "\n",
        "* **Strong Sales in 2014** - There is noticeable increase in sales during 2014 compared to the preceding & subsequent years .this could indicate a period of significant growth or successful marketing initiatives during that year.\n",
        "\n",
        "* **Correlation between Sales & Sales Volume** - There appears to be a positive correlation between sales and sales volume, as the general trend of increasing sales volume aligns with the overall pattern of sales."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insight have the potential to create a positive business impact by leveraging seasonal sales trends, increasing sales volume & identifying success"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 Median Sales by store Type/Assortment Over Year & Month."
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a copy of the dataframe\n",
        "df_temp = df_m.copy()\n",
        "\n",
        "# Extract year and month from Date.\n",
        "df_temp['year'] = pd.to_datetime(df_temp['Date']).dt.year\n",
        "df_temp['month'] = pd.to_datetime(df_temp['Date']).dt.month\n",
        "\n",
        "# Groupby year, month & storetype, then find the median of sales.\n",
        "sales_data = df_temp[['Sales', 'StoreType', 'year', 'month']].groupby(['year', 'month', 'StoreType']).median()\n",
        "\n",
        "# Reshape the data for plotting\n",
        "sales_data = sales_data.unstack(level='StoreType')\n",
        "\n",
        "# Plotting\n",
        "sales_data.plot(figsize=(10, 6))\n",
        "\n",
        "plt.title('Median Sales by Store Type Over Year & Month')\n",
        "plt.xlabel('Time (year, Month)')\n",
        "plt.ylabel('Median Sales')\n",
        "plt.show()\n",
        "\n",
        "##-------------------------------------------------------------------------------------------------------------------------##\n",
        "\n",
        "# Groupby year, month & Storetype, then find the median of sales.\n",
        "sales_data = df_temp[['Sales', 'Assortment', 'year', 'month']].groupby(['year', 'month', 'Assortment']).median()\n",
        "\n",
        "# Reshape the data for plotting\n",
        "sales_data = sales_data.unstack(level='Assortment')\n",
        "\n",
        "# Plotting.\n",
        "sales_data.plot(figsize=(10, 6))\n",
        "plt.title('Median Sales by Assortment Over Year & Month')\n",
        "plt.xlabel('Time (Year, Month)')\n",
        "plt.ylabel('Median Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Line plot help us to identify the trend in the data and compare multiple categories."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Overall Trend** - There is a general growth in sales across all store type & assortment from 2013 to 2015.\n",
        "* **Store Type Performance** - Store type \"b\" consistently has the highest sales throughout the entire period, followed by assortment type \"d\". Store types \"a\" & \"c\" generally has lower sales numbers in comparison.\n",
        "* **Assortment Performance** - Assortment type \"c\" consistently has the highest sales throughout the entire period, followed by assortment type \"b\". Assortment type \"a\" generally has lower sales numbers in comparison."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These insights can help businesses identify the best-performing store type & assortments and allocate resources accordingly."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 Median Sales with/without Promo/Promo2 Over Year & Month."
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Create a copy of the dataframe\n",
        "df_temp = df_m.copy()\n",
        "\n",
        "# Extract year and month from the Date.\n",
        "df_temp['year'] = pd.to_datetime(df_temp['Date']).dt.year\n",
        "df_temp['month'] = pd.to_datetime(df_temp['Date']).dt.month\n",
        "\n",
        "# Group by year, month & Storetype then find the median of sales.\n",
        "sales_data = df_temp[['Sales', 'Promo', 'month', 'year']].groupby(['year', 'month', 'Promo']).median()\n",
        "\n",
        "# Reshape the data for plotting.\n",
        "sales_data = sales_data.unstack(level='Promo')\n",
        "\n",
        "# Plotting.\n",
        "sales_data.plot(figsize=(10, 6))\n",
        "\n",
        "plt.title('Median Sales with/without Promo Over Year & Month')\n",
        "plt.xlabel('Time (Year, Month)')\n",
        "plt.ylabel('Median Sales')\n",
        "plt.show()\n",
        "\n",
        "##------------------------------------------------------------------------------------------------------------------##\n",
        "\n",
        "# Group by year, month & Storetype then find the median of sales.\n",
        "sales_data = df_temp[['Sales', 'Promo2', 'month', 'year']].groupby(['year', 'month', 'Promo2']).median()\n",
        "\n",
        "# Reshape the data for plotting\n",
        "sales_data = sales_data.unstack(level='Promo2')\n",
        "\n",
        "# Plotting.\n",
        "sales_data.plot(figsize=(10, 6))\n",
        "\n",
        "plt.title('Median Sales with/without promo2 over Year & Month')\n",
        "plt.xlabel('Time (Year, Month)')\n",
        "plt.ylabel('Median Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A Line plot help us to identify the trends in the data & compare multiple categories."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Promotion Impact** -\n",
        "    1. The Sales data shows that promotional peroids(promo = 1) generally result in higher sales compared to non-promotional (promo=0) across all years.\n",
        "    2. The impact of promo2 is less on the sales where promo2=0 has higher sales."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Promotional Strategy Evaluation** - By analyzing the sales performance during promotional & non-promotional periods, businesses can assess the effectiveness of their promotional strategies."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 Average Number of Customer Over School Holiday & Promo."
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Create a subset of the data with the required columns.\n",
        "subset_df = df_m[['Customers', 'Promo', 'SchoolHoliday']]\n",
        "\n",
        "# Group the data by 'Promo' & 'SchoolHoliday' and calculate the average number of customers.\n",
        "grouped_data = subset_df.groupby(['Promo', 'SchoolHoliday'])['Customers'].mean().reset_index()\n",
        "\n",
        "# Pivot the data to make it suitable for visualization.\n",
        "pivot_table = grouped_data.pivot(index='Promo', columns = 'SchoolHoliday', values= 'Customers')\n",
        "\n",
        "# Plotting the customer count using a heatmap.\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(pivot_table, cmap='viridis', aspect='auto')\n",
        "\n",
        "# Set x-axis and y-axis labels.\n",
        "plt.xlabel('School Holiday')\n",
        "plt.ylabel('Promotional Activities')\n",
        "\n",
        "# Set x-tick labels.\n",
        "plt.xticks([0, 1], ['No', 'Yes'])\n",
        "plt.yticks([0, 1], ['No', 'Yes'])\n",
        "\n",
        "# Add a color bar legend\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Average Number of Customers')\n",
        "\n",
        "# Add value annotations to each cell\n",
        "for i in range(pivot_table.shape[0]):\n",
        "   for j in range(pivot_table.shape[1]):\n",
        "     plt.text(j,i,f'{pivot_table.iloc[i, j]:.2f}', ha='center', va='center', color='white')\n",
        "\n",
        "# Add a title.\n",
        "plt.title('Average Number of Customers: Promo VS School Holiday')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here a heatmap is used to visualize patterns & relationships in data that are organized in a tabular format."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The combination of a promotion & non-school holiday has the highest average number of customers (824.27).\n",
        "* On the other hand, the combination of no promotion & non-school holiday has the lowest average number of customers (498.24)"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These insights suggest that promotions play a significant role in increasing customer visits, particularly during non-school holidays."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Correlation Heatmap\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Plot correlation matrix, specify image size.\n",
        "corr_matrix = df_m2.corr()\n",
        "fig, ax = plt.subplots(figsize= (12, 9))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation matrix, can be used to show the correlation coefficient between pairs of variable in a dataset. correlation coefficients can range from -1 to 1.\n",
        " * A Correlation of 1 indicates a perfect positive correlation. That is for every increase in one variable, there is a proprotinate increase in the other."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The strongest positivie correlation appears to be between Sales & Customers (0.82). This is to be expected, as more customers would generally lead to higher sales.\n",
        "2. The variable Promo has a significant positive correlation with both Sales(0.37) & Customers(0.18).This suggests that running a promotion tend to increase both sales and customer visits.\n",
        "3. Promo2 seems to have a siginificant negative correlation with both Sales(-0.13)and Customer (-0.20) which might indicate that this type of promotion does not work as effectively as Promo in driving sales and customer visits or even has a negative effect.\n",
        "4. CompetitionDistance Show a negative correlation with Customers (-0.15) suggesting that store with closer competitiors might have more customers.\n",
        "5. Promo2SinceYear has notable negative correlation with promo2SinceWeek (-0.24), Since they might be derrived from the same date.  "
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1 The Presence of a promotion(Promo)has a positive impact on sales."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null Hypothesis - H0: The presence of a promotion(promo) has no siginificant impact on sales.\n",
        "* Alternate Hypothesis- HA: The presence of a promotion(Promo)has an impact on sales."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Seperate the data into two groups.\n",
        "promo_true = df_m2[df_m2['Promo']== 1]\n",
        "promo_false = df_m2[df_m2['Promo']== 0]"
      ],
      "metadata": {
        "id": "HvLxTHtnQy9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "t_stat, p_value = stats.ttest_ind(promo_true['Sales'], promo_false['Sales'], axis=0, equal_var=True, nan_policy='propagate', permutations=None, random_state=None, alternative='two-sided', trim=0)\n",
        "print('t-stat:', t_stat)\n",
        "print('p-value:', p_value)\n",
        "\n",
        "# Compare p_value with the siginificant level (0.05)\n",
        "if p_value < 0.05:\n",
        "   conclusion = \"Reject Null hypothesis. The presence of a promotion (promo) has an impact on sales.\"\n",
        "else:\n",
        "    conclusion = \"Fail to Reject Null hypothesis. The presence of a promotion (promo) has no siginificant impact on sales.\"\n",
        "\n",
        "print(conclusion)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The independent t-test, also called the two sample t-test,independent-samples-t-test or student's t-test, is a inferential statistical test that determines whether there is a statistically significant difference between the means in two unrelated groups."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test? Insights from the Hypothesis Testing."
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The t-statistic value(t=363.84) indicates the magnitude of the difference between the means of two groups. A higher absolute value of the t-statistic indicates a larger difference between the means.\n",
        "\n",
        "* The t-test result produced a very low p_value(p_value=0.0), which means we can safely reject null hypothesis. This suggest there is a significant difference in sales between the days where there is promotion(promo=1) and the days when there is no promotion(Promo=0)."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2 There is significant difference in sales between different store types."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null hypothesis(H0)- There is no significant difference in sales between different store types.\n",
        "\n",
        "* Alternative hypothesis(H1) - There is a significant difference in sales between different store type."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# perform ANOVA test.\n",
        "f_statistic, p_value = stats.f_oneway(\n",
        "    df_m2['Sales'][df_m2.StoreType == 'a'],\n",
        "    df_m2['Sales'][df_m2.StoreType == 'b'],\n",
        "    df_m2['Sales'][df_m2.StoreType == 'c'],\n",
        "    df_m2['Sales'][df_m2.StoreType == 'd'])\n",
        "\n",
        "print('f-statistic:', f_statistic)\n",
        "print('p-values:', p_value)\n",
        "\n",
        "# Compare p-value with significance level (0.05)\n",
        "if p_value < 0.05:\n",
        "   conclusion = \"Reject Null hypothesis. There is a significant difference in sales between different store types. \"\n",
        "else:\n",
        "   conclusion = \"Fail to Reject Null hypoythesis. There is no significant difference in sales between different store types.\"\n",
        "\n",
        "print(conclusion)"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA is a statistical test used to compare the means of two or more groups to determine if there are significant difference between them. In this case the different store type are treated, as the groups & the sales data is compared to see if there are significant variations among the groups.\n",
        "By Choosing the ANOVA test we can compare the means of multipale store types simultaneously, rather than conducting pairwise comparisons between each pair of store types."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test? Insights from hypothesis test."
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-statistic(F=6081) is measure of the ratio of variance between groups to the variance within groups. A higher F-statistic suggests a large difference between the group means relative to the variability within each group.\n",
        "\n",
        "A p_value below certain threshold (commonly 0.05) indicates that the observed differences are unlikely to be due to random chance.hence in this case (p=0.0) we can reject the null hypothesis. There is significat difference in sales between different store types."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3  There is significant difference in sales on different days of the week."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null hypothesis (H0) - There is no significant difference in sales on different days of the week.\n",
        "\n",
        "* Alternative hypothesis (H1) - There is a significant difference in sales on different days of the week."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Preform one-way ANOVA test.\n",
        "f_statistic, p_value = stats.f_oneway(\n",
        "    df_m2[df_m2['DayOfWeek']== 1]['Sales'],\n",
        "    df_m2[df_m2['DayOfWeek']== 2]['Sales'],\n",
        "    df_m2[df_m2['DayOfWeek']== 3]['Sales'],\n",
        "    df_m2[df_m2['DayOfWeek']== 4]['Sales'],\n",
        "    df_m2[df_m2['DayOfWeek']== 5]['Sales'],\n",
        "    df_m2[df_m2['DayOfWeek']== 6]['Sales'],\n",
        "    df_m2[df_m2['DayOfWeek']== 7]['Sales'])\n",
        "\n",
        "print('f-statistic:', f_statistic)\n",
        "print('p_value:', p_value)\n",
        "\n",
        "# Compare the p_value with significant level(0.05)\n",
        "if p_value < 0.05:\n",
        "   conclusion = \"Reject Null hypothesis. There is a significant difference in sales on different days of the week.\"\n",
        "else:\n",
        "    conclusion = \"Fail to Reject Null hypothesis. There is no significant difference in sales on different days of the week.\"\n",
        "\n",
        "print(conclusion)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA is a statistical test used to compare the means of two or more groups to determine if there are significant difference between them."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test? Insight from hypothesis test."
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The large F-statistic suggests (F=7451) a large difference between the group means relative to the variability within each group.\n",
        "\n",
        "The p_value (p=0.0) indicates that the observed difference are unlikely to be due to random chance, hence in this case (p=0.0) so we can reject the null hypothesis. ie, there is a significant difference in sales on different days of the week."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Let's identify the rows with missing values in CompetitonsDistance."
      ],
      "metadata": {
        "id": "0l5MnrxiiD7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_m2.shape"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_m2[np.isnan(df_m2['CompetitionDistance'])].isna().sum()"
      ],
      "metadata": {
        "id": "rkC070fmikg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can assum the rows with missing values in the competition distance have no competition. Hence we can fill the rows with competition details with a value outside the range."
      ],
      "metadata": {
        "id": "DDV9TyOcjFa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating max value of competition open since date from year & month.\n",
        "df_temp = df_m2.dropna().copy()\n",
        "pd.to_datetime(\n",
        "    df_temp['CompetitionOpenSinceYear'].astype(int).astype(str) + '-' +\n",
        "    df_temp['CompetitionOpenSinceMonth'].astype(int).astype(str) + '-1'\n",
        ").max()"
      ],
      "metadata": {
        "id": "BVMfL7WcjfJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the max value of competition distance.\n",
        "df_temp['CompetitionDistance'].max()"
      ],
      "metadata": {
        "id": "uvdrZh1JkcTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of the original Dataframe.\n",
        "df_c1 = df_m2.copy()\n",
        "\n",
        "# Specifying the column to impute.\n",
        "columns_to_impute = ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear']\n",
        "\n",
        "# Identifying the rows where CompetitionDistance is missing.\n",
        "condition = df_c1['CompetitionDistance'].isnull()\n",
        "\n",
        "# Update the specified columns with a value outside the range for the rows satisfying the condition.\n",
        "df_c1.loc[condition,'CompetitionDistance'] = 4000\n",
        "df_c1.loc[condition,'CompetitionOpenSinceMonth'] = 12\n",
        "df_c1.loc[condition, 'CompetitionOpenSinceYear'] = 2016\n"
      ],
      "metadata": {
        "id": "5dCus3UgkpWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 let impute the remaining missing values with either mode or median"
      ],
      "metadata": {
        "id": "xcu2YhsQmm9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the columns to impute.\n",
        "columns_to_impute_medain = ['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
        "columns_to_impute_mode = ['PromoInterval', 'Promo2']\n",
        "\n",
        "df_c1_imputed = df_c1.copy()\n",
        "# Impute the missing values using mode\n",
        "for column in columns_to_impute_mode:\n",
        "  df_c1_imputed[column].fillna(df_c1_imputed[column].mode()[0], inplace=True)\n",
        "\n",
        "# Impute the missing values using median.\n",
        "for column in columns_to_impute_medain:\n",
        "  df_c1_imputed[column].fillna(df_c1_imputed[column].median(), inplace=True)"
      ],
      "metadata": {
        "id": "zUtOtlmxmyvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_c1_imputed.shape"
      ],
      "metadata": {
        "id": "jghumKrmpU48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we used the mode & median imputation.\n",
        "\n",
        "1. ** Mode Imputation**: Mode imputation is used to fill in missing categorical or nominal data.The Mode is the most frequently occurring value in a dataset. when a value is missing.it can be replaced with the mode of that particular feature.This approach assumes that the missing value is likely to be similar to the most common value in  the dataset.\n",
        "\n",
        "2. **Median Imputation**: Median imputation is used to fill in missing numerical or continuous data.The median is the middle value in a sorted list of numbers. When a value is missing, it can be replaced with the median of that particular feature.This approach assumes that the missing value is likely to be similar to the typical or central value in the dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "def cap_outliers(df):\n",
        "  # create a copy of the dataframe to avoid modifying the orignial data.\n",
        "  cleaned_df = df.copy()\n",
        "\n",
        "  # Iterate over numerical columns.\n",
        "  for column in cleaned_df.select_dtypes(include = np.number):\n",
        "    # Calculate the 99th percentil value.\n",
        "    upper_cap_value = cleaned_df[column].quantile(0.99)\n",
        "\n",
        "    # Caping lower outliers seems to decrease model score, hence a commented out.\n",
        "    # lower_cap_value = cleaned_df[column].quantile(0.05)\n",
        "\n",
        "    # Cap outliers to the 99th percentile value.\n",
        "    cleaned_df[column] = np.where(cleaned_df[column] > upper_cap_value, upper_cap_value, cleaned_df[column])\n",
        "    # cleaned_df[column] = np.where(cleaned_df[column] < lower_cap_value, lower_cap_value, cleaned_df[column])\n",
        "\n",
        "  return cleaned_df\n",
        "df_c1_capped = cap_outliers(df_c1_imputed)"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Since we are dealing with retail data which might have spikes in sales during peak seasons, we can take into account most of the values in the sales column & customer column.\n",
        "* How ever outliers in the remaining columns needs to be addressed.\n",
        "* For simplicity, we cap the values in all columns upto the respective 99th percentile values."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "PGzX14xidUaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "bPVV0_Qldn3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Creating new feature from Date column."
      ],
      "metadata": {
        "id": "WTPxWb4Pdv56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulating Features to minimize feature correlation & create new feature.\n",
        "# Create week,month & year columns from Date.\n",
        "df_c1_capped['week_number'] = (df_c1_capped['Date']).dt.week\n",
        "df_c1_capped['month'] = (df_c1_capped['Date']).dt.month\n",
        "df_c1_capped['year'] = (df_c1_capped['Date']).dt.year\n",
        "\n"
      ],
      "metadata": {
        "id": "bdKYILkMd4yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Creating new feature IspromoMonth."
      ],
      "metadata": {
        "id": "aRo-zL4PfTMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a column to detect whether a promo run in the current month.\n",
        "df_c1_capped['IsPromoMonth'] = df_c1_capped.apply(lambda row: 1 if row['Date'].strftime('%b') in row['PromoInterval'] else 0, axis=1)\n",
        "df_c1_capped['IsPromoMonth'].sample(5)"
      ],
      "metadata": {
        "id": "ssYIA9vSfeMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Create new feature Av_Sales/Assortment/StoreType/Month & Av_customers/Assortment/StoreType/Month."
      ],
      "metadata": {
        "id": "jE585R2Nhr1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Av_sales/Assorment/StoreType/Month.\n",
        "describe_1 = df_c1_capped.groupby(['StoreType','Assortment','month'])['Sales'].mean().reset_index()\n",
        "describe_1.rename(columns={'Sales': 'Av_sales/Assortment/StoreType/Month'}, inplace=True)\n",
        "describe_1.sample(5)"
      ],
      "metadata": {
        "id": "NKyq3_-zipB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Av_customer/Assortment/StoreType/Month.\n",
        "describe_2 = df_c1_capped.groupby(['StoreType','Assortment','month'])['Customers'].mean().reset_index()\n",
        "describe_2.rename(columns={'Customers': 'Av_customers/Assortment/StoreType/Month'}, inplace=True)\n",
        "describe_2.sample(5)"
      ],
      "metadata": {
        "id": "XTjPYMezkIDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge tha above two tables.\n",
        "describe = describe_1.join(describe_2['Av_customers/Assortment/StoreType/Month'])\n",
        "describe.dropna(inplace=True)\n",
        "print(describe.shape)\n",
        "describe.sample(5)\n"
      ],
      "metadata": {
        "id": "bVdji614lumr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Av_sales/Assortment/StoreType/Month, Av_customer/Assortment/StoreType/Month information to our dataset.\n",
        "df_c2_capped = df_c1_capped.merge(describe, on=['StoreType', 'Assortment','month'])\n",
        "print(df_c2_capped.shape)\n",
        "df_c2_capped.sample(5)"
      ],
      "metadata": {
        "id": "s6eohyfemeJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4 Convert competition Distance to binned values."
      ],
      "metadata": {
        "id": "GAHly6zvnpc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_c2_capped['CompetitionDistanceBin'] = pd.cut(df_c2_capped['CompetitionDistance'],6, labels=['1','2','3','4','5','6'])"
      ],
      "metadata": {
        "id": "cN-SeTT-nzEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.5 Create a new feature 'CompetitonOpenSinceDay'."
      ],
      "metadata": {
        "id": "lalozXvooSpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp = df_c2_capped.copy()\n",
        "\n",
        "# Create a CompetitionOpenSinceDay' from year & month values.\n",
        "df_temp['CompetitionOpenSinceDay'] = (pd.to_datetime(\n",
        "    df_temp['CompetitionOpenSinceYear'].astype(int).astype(str) + '-' +\n",
        "    df_temp['CompetitionOpenSinceMonth'].astype(int).astype(str) + '-1'\n",
        ")-df_temp['Date']).dt.days\n",
        "\n",
        "##---------------------------------------------------------------------------------------##\n",
        "\n",
        "# Handling Outliers.\n",
        "upper_cap = df_temp['CompetitionOpenSinceDay'].quantile(0.95)\n",
        "lower_cap = df_temp['CompetitionOpenSinceDay'].quantile(0.05)\n",
        "\n",
        "df_temp2 = df_temp.copy()\n",
        "# replace outliers with capped value.\n",
        "df_temp2['CompetitionOpenSinceDay'] = np.where(df_temp['CompetitionOpenSinceDay'] > upper_cap, upper_cap, df_temp['CompetitionOpenSinceDay'])\n",
        "df_temp2['CompetitionOpenSinceDay'] = np.where(df_temp['CompetitionOpenSinceDay'] < lower_cap, lower_cap, df_temp['CompetitionOpenSinceDay'])\n",
        "\n",
        "##--------------------------------------------------------------------------------------------##\n",
        "\n",
        "df_temp2['Promo2SinceWeek'] = df_temp2['Promo2SinceWeek'].astype(int)\n",
        "df_temp2['Promo2SinceYear'] = df_temp2['Promo2SinceYear'].astype(int)\n",
        "\n",
        "# Create a Promo2SinceDay from year & month value.\n",
        "df_temp2['Promo2SinceDay'] = (pd.to_datetime(df_temp2['Promo2SinceYear'].astype(str),format='%Y') + \\\n",
        "                              pd.to_timedelta(df_temp2['Promo2SinceWeek'] * 7, unit='days')-df_temp2['Date']).dt.days\n",
        "\n",
        "# Check the shape of the new dataframe.\n",
        "print(df_c2_capped.shape)\n",
        "print(df_temp2.shape)\n",
        "\n",
        "# Copy the changed dataframe to df_c3_capped.\n",
        "df_c3_capped = df_temp2.copy()"
      ],
      "metadata": {
        "id": "hSZW983QoeTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features & target.\n",
        "cols_to_drop = ['Sales', 'Date', 'Store']\n",
        "target_col = ['Sales']\n",
        "X = df_c3_capped.drop(cols_to_drop, axis=1)\n",
        "y = df_c3_capped[target_col]\n",
        "\n",
        "# Perform one-hot encoding for features.\n",
        "X_encoded = pd.get_dummies(X)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_encoded.info()"
      ],
      "metadata": {
        "id": "B06T7Ydyu-Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we used pd.get_dummies() which is a function provided by the pandas library in python. it's used for one-hot encoding categorical variables or features.One-hot encoding is a process of transforming categorical variable into binary vector, making them suitable for machine learning algorithms."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "import numpy as np\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
        "\n",
        "def calculate_vif(df):\n",
        "  # select numerical columns.\n",
        "  numerical_columns = df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "  # Create a dataframe to store VIF result.\n",
        "  vif_data = pd.DataFrame()\n",
        "  vif_data['Variable'] = numerical_columns.columns\n",
        "  vif_data['VIF'] = [vif(numerical_columns.values, i) for i in range(numerical_columns.shape[1])]\n",
        "\n",
        "  return vif_data.sort_values(by='VIF', ascending=False)\n",
        "\n",
        "calculate_vif(X_encoded.drop(['year', 'Promo2SinceYear', 'Av_sales/Assortment/StoreType/Month', 'Customers', 'CompetitionOpenSinceYear', 'month'], axis=1))"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select top 25 features based on mutual_info_regression.\n",
        "selector = SelectKBest(mutual_info_regression, k=25)\n",
        "X = X_encoded.drop(['year', 'Promo2SinceYear', 'Av_sales/Assortment/StoreType/Month', 'Customers', 'CompetitionOpenSinceYear', 'month'], axis=1).copy()\n",
        "y = y\n",
        "X_new = selector.fit_transform(X, y)\n",
        "selected_features = X.columns[selector.get_support()].to_list()\n",
        "\n",
        "print('Selected features:', selected_features)"
      ],
      "metadata": {
        "id": "460HbF3q0Nmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Variance Inflation Factor**(VIF) is a statistical concept used to measure the severity of multicolinearity in regression analysis.\n",
        "Multicolinearity occurs when there is a correlation between multiple independent variable in a multiple regression model, which can adversely affect the regression results.\n",
        "\n",
        "2. **SelectKBest** is a feature selection method provided by the sklearn.feature_selection module in the scikit-learn library. it is used to select the top k feature from a given dataset based on a specific scoring function. One of the scoring functions available in scikit-learn is Mutul information Regression.\n",
        "\n",
        "* Mutual information Regression: This is a technique used to measure the dependency or information gain between two variables. it is commonly used in feature selection for regression task.Mutual information captures both linear & non-linear relationships between variables."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As from the EDA we can see that sales vary greatly with promo.\n",
        "* Competition Distance, StoreType & Assortment type also has some influence on sales.\n",
        "* We are not using the Customer column as it wont be available beforehand & since our objective is to forcast sales.\n",
        "\n",
        "For training the model we can use the feature selected using SelectKBest."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Splitting"
      ],
      "metadata": {
        "id": "1cTTgHaXZRQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll select a test set from our dataset that is 6 weeks long for evaluating our model on unseen data."
      ],
      "metadata": {
        "id": "8XIQnqD_ZVg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll select the test set from our dataset that is 6 week long for evaluating our model on unseen data.\n",
        "df_c1_train_val = df_c3_capped[df_c2_capped['Date'] < (pd.to_datetime('2015-07-31') - pd.to_timedelta(42, unit='d'))]\n",
        "df_c1_test = df_c3_capped[df_c2_capped['Date'] >= (pd.to_datetime('2015-07-31') - pd.to_timedelta(42, unit='d'))]\n",
        "\n",
        "# We are dropping the Date & Store Columns since it contains a lot of classes and might be cause of overfitting.\n",
        "cols_to_drop = ['Sales', 'Date', 'Store']\n",
        "target_col = ['Sales']\n",
        "\n",
        "# Select feature & target for training & validation.\n",
        "X_train_val = df_c1_train_val.drop(cols_to_drop, axis=1)\n",
        "y_train_val = df_c1_train_val[target_col]\n",
        "\n",
        "# Select feature & target for testing.\n",
        "X_test = df_c1_test.drop(cols_to_drop, axis=1)\n",
        "y_test = df_c1_test[target_col]"
      ],
      "metadata": {
        "id": "dnRJp83-ZvPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of the test set.\n",
        "test_size_perc = len(df_c1_test)/ len(df_c3_capped)*100\n",
        "print(f'allocated percentage for test set: {test_size_perc:.2f}%')"
      ],
      "metadata": {
        "id": "PLphWlE9cSFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the features\n",
        "X_train_val_selected = pd.get_dummies(X_train_val)[selected_features]\n",
        "X_test = pd.get_dummies(X_test)[selected_features]\n",
        "\n",
        "# Split data into train & test\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val_selected, y_train_val, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "2wU5JITxc-H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_val_selected.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "Cv290YgYd_lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "WINCQyX7eRvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Since we have sufficiently large dataset, we can use 80% of the data for training & 20% for validation.\n",
        "* Allocating 20% of the data for validation provides a separate set of examples to evaluate the model's performance."
      ],
      "metadata": {
        "id": "IV3S1kuDeV8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Apply square root transformation to target variable.\n",
        "y_train_s = np.sqrt(y_train)\n",
        "y_val_s = np.sqrt(y_val)\n",
        "y_test_s = np.sqrt(y_test)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here square root transformation is used on target variables because it exhibit a slightly skewed distribution.\n",
        "* it can reduce the impact of extreme values & make the data more symmetrical."
      ],
      "metadata": {
        "id": "eovxv4Lmf0s4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = MinMaxScaler()\n",
        "X_train_s = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "X_val_s = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
        "X_test_s = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have used MinMaxScaler.\n",
        "**MinMaxScaler** is used to scale numerical features in a dataset to a specific range using each feature's minimum & maximum value.\n",
        "\n",
        "* The purpose of using MinMaxScaler is to bring all the features to a similar scale, which can be important for certain machine learning algorithms. Scaling the features ensures that no particular feature dominates the learning process due to differences in their scales."
      ],
      "metadata": {
        "id": "wtW48xJqicjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our number of feature selected is only 25, dimensionality reduction is not required."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the optimum models for our dataset, we will train and compare the evaluation metrics of multiple models ar once."
      ],
      "metadata": {
        "id": "GMm23N9DkcHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import time\n",
        "from time import time as timer\n",
        "\n",
        "# Define models.\n",
        "models = [\n",
        "    ('Linear Regression', LinearRegression()),\n",
        "    ('Decision Tree Regression', DecisionTreeRegressor(random_state=42)),\n",
        "    ('Random Forest Regression', RandomForestRegressor(random_state=42)),\n",
        "    ('Gradient Boosting Regression', GradientBoostingRegressor(random_state=42)),\n",
        "    ('XGBoosting Regression', XGBRegressor(random_state=42))\n",
        "]\n",
        "\n",
        "def Train_evaluate(models, X_train, y_train, X_val, y_val):\n",
        "   \"\"\"\n",
        "   Train and evaluate regression models using different valuation metrics.\n",
        "\n",
        "   Args:\n",
        "       models (list): List of tuple  containing the model names & model instances.\n",
        "       X_train (array-like) : Training data features.\n",
        "       y_train (array-like) : Training data targets.\n",
        "       X_val (array-like) : valuation data features.\n",
        "       y_val (array-like) : valuation data targets.\n",
        "\n",
        "   Returns:\n",
        "       Tuple (model, metrics_df): Trained model and metrics dataframe.\n",
        "   \"\"\"\n",
        "   # Initialize lists to store the metrics\n",
        "   metrics = []\n",
        "\n",
        "   # Const numbers to calculate adjusted R-squared\n",
        "   p_train = X_train.shape[1]\n",
        "   n_train = y_train.shape[0]\n",
        "\n",
        "   p_val = X_val.shape[1]\n",
        "   n_val = y_val.shape[0]\n",
        "\n",
        "   # Train & evalute each model.\n",
        "   for name, model in models:\n",
        "      # fit & predict using the selected model\n",
        "      start_train_time = timer()\n",
        "      model.fit(X_train, y_train)\n",
        "      end_train_time = timer()\n",
        "      training_time = end_train_time - start_train_time\n",
        "\n",
        "      start_pred_time = timer()\n",
        "      y_pred_train = model.predict(X_train)\n",
        "      y_pred_val = model.predict(X_val)\n",
        "      end_pred_time = timer()\n",
        "      prediction_time = end_pred_time - start_pred_time\n",
        "\n",
        "      # Evaluation metrics for train data.\n",
        "      rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "      mae_train = mean_absolute_error(y_train, y_pred_train)\n",
        "      r2_train = r2_score(y_train, y_pred_train)\n",
        "      adjusted_r2_train = 1- ((1-r2_train) * (n_train - 1)) / (n_train - p_train - 1)\n",
        "      mape_train = (mae_train / np.mean(y_train)) * 100\n",
        "\n",
        "      # Evaluation metrics for val data\n",
        "      rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
        "      mae_val = mean_absolute_error(y_val, y_pred_val)\n",
        "      r2_val = r2_score(y_val, y_pred_val)\n",
        "      adjusted_r2_val = 1- ((1 - r2_val) * (n_val - 1)) / (n_val - p_val - 1)\n",
        "      mape_val = (mae_val / np.mean(y_val)) * 100\n",
        "\n",
        "\n",
        "      # Append valuation metrics to metrics list\n",
        "      metrics.append([\n",
        "          name, rmse_train, mae_train, r2_train, adjusted_r2_train, mape_train[0],\n",
        "          rmse_val, mae_val, r2_val, adjusted_r2_val, mape_val[0], training_time, prediction_time\n",
        "      ])\n",
        "\n",
        "   # Create a dataframe for metrics\n",
        "   metrics_df = pd.DataFrame(\n",
        "   metrics,\n",
        "    columns=['Model', 'RMSE Train', 'MAE Train', 'R-squared Train', 'Adjusted R-squared Train', 'MAPE Train',\n",
        "             'RMSE val', 'MAE val', 'R-squared val', 'Adjusted R-squared val', 'MAPE val', 'Training Time', 'Prediction Time']\n",
        "   )\n",
        "\n",
        "   # Format numeric columns\n",
        "   metrics_df = metrics_df.round(2)\n",
        "\n",
        "   return metrics_df, models"
      ],
      "metadata": {
        "id": "nveRyTOQkyl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_history,_ = Train_evaluate(models, X_train_s, y_train_s, X_val_s, y_val_s)\n",
        "metrics_history"
      ],
      "metadata": {
        "id": "Bmdl4q0OxuPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here linear regression is only able to explain 25% of variance in the target variable. Since it fails to capture the complexity of the data, we will use the rest of the models for our following analysis."
      ],
      "metadata": {
        "id": "xE55FrtXBj3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 Decision Tree."
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A Decision tree is a non-parametric supervised learning algorithm used for both classification & regression task.it has a hierarchical, tree-like structure cosisting of a root node, branches, internal nodes & leaf nodes.\n",
        "\n",
        "* In a decision tree, each internal node represents a \"test\"on an attribute each branch represents the outcome of the test, and each leaf node represents a class label or decision taken after computing all attributes.The paths from the root to the leaf nodes represent classification rules.\n",
        "\n",
        "* Decision Tree are commonly used in operations research, decision analysis & machine learning."
      ],
      "metadata": {
        "id": "ya2PrkHVrH-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics_history[metrics_history['Model'] == 'Decision Tree Regression']"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Decision Tree Regression model seems to overfit the training data indicated by the R-squared scores of 1.0 on the training set.The model performs reasonably well on validation data, suggested by R-squared score of 0.81 on validation set.\n",
        "\n",
        "* Overall, the Decision Tree Regeression model shows promising performance & can be improved by tunning hyperparameters."
      ],
      "metadata": {
        "id": "A2xO6YX1ww2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here we have used Optuna library for hyperparameter optimization for all ML models.\n",
        "* Optuna is hyperparameter Optimization software framework designed for machine lerning. it allows user to implement different state-of-the-art optimization methods to perform hyperparameter optimization rapidly with great performance.\n",
        "* By default, optuna implements a Bayesian optimization- Tree structured Parzen Estimator (TPE) algorithm. TPE constructs a probabilistic model to model the relationship between hyperparameters & the objective function and the samples the hyperparameters based on this model to guide the search towards promising regions of the hyperparameter space.\n",
        "*In the following code, the objective function defines the objective to be maximmized, which is the R-squared score between the predicted and actual values.The trial object is used to sample hyperparameters from a specified search space."
      ],
      "metadata": {
        "id": "tOfukFl24AOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        'max_depth': 55, # Model performance increase with depth, also causes overfitting\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 4, 10), # Initial trail used 2,10 then narrowed to current value.\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 9, 18), # Initial trial used 1, 10 then narrowed to current value.\n",
        "        'criterion': 'friedman_mse'  # Eliminated {poisson, 'absolute_error}\n",
        "    }\n",
        "\n",
        "    model = DecisionTreeRegressor(**params)\n",
        "\n",
        "    model.fit(X_train_s, y_train_s)\n",
        "    val_r2 = model.score(X_val_s, y_val_s)\n",
        "    print('Train r2 score: ', model.score(X_train_s, y_train_s))\n",
        "\n",
        "    return val_r2\n",
        "\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=15)\n",
        "\n",
        "print('Best hyperparameters: ', study.best_trial.params)\n",
        "print('Best r2 Score: ', study.best_value)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "| max_depth | min_samples_split | min_samples_leaf | criterion      | R2 Score     |\n",
        "|----|------------------|-----------------|----------------|--------------|\n",
        "| 65 | 9                | 10              | friedman_mse   | 0.860358     |\n",
        "| 55 | 8                | 10              | friedman_mse   | 0.860386     |\n",
        "| 45 | 4                | 10              | friedman_mse   | 0.860393     |\n",
        "| 45 | 4                | 6               | friedman_mse   | 0.858035     |\n",
        "| 35 | 5                | 5               | friedman_mse   | 0.856207     |\n",
        "| 25 | 3                | 5               | friedman_mse   | 0.845925     |\n",
        "| 15 | 3                | 4               | friedman_mse   | 0.643338     |"
      ],
      "metadata": {
        "id": "AbQFvMZjv9p6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As we can see from above result increasing the depth of tree leads to higher optimized result, however increasing from n=45 and higher leads to only marginal improvement .\n",
        "* Hence we can conclude that the model coverges at hyperparameters{'max_depth' = 45, min_samples_split= 4, min_sample_leaf=10 and criterion = friedman_mse}.\n",
        "* We can stop the trial here since increasing the depth of the tree might lead to overfitting & increasing the model complexity."
      ],
      "metadata": {
        "id": "-r85cIQBwZTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected hypreparameters for Decision Tree Regression\n",
        "params = {'max_depth' : 45, 'min_samples_split' : 4, 'min_samples_leaf' : 10, 'criterion' : 'friedman_mse'}\n",
        "\n",
        "# use the best model to make predictions.\n",
        "models = [('Decision Tree Regression HPO', DecisionTreeRegressor(**params,random_state=42))]\n",
        "metrics_df,_ = Train_evaluate(models, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Save evaluation metrics to metrics_history\n",
        "metrics_history = metrics_history.append(metrics_df)\n",
        "metrics_history[(metrics_history['Model'] == 'Decision Tree Regression') |\n",
        "                (metrics_history['Model'] == 'Decision Tree Regression HPO')]"
      ],
      "metadata": {
        "id": "oKmJKmrUyLlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The HPO has lead to more optimized results indicated by lower R-squared score of 0.9 on the training set and a higher R-squared score of 0.85 on the validation set.\n",
        "* The model is able to generalize better on unseen data after the hyperparameter tuning ."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2  Random Forest."
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sklearn.ensemble.RandomForestRegressor.**\n",
        "\n",
        "* Random forest is machine learning algorithm that is uses for classification & regression tasks. it is an ensemble learning method that combines multiple decision tree to make prediction.Each individual tree in the random forest produces a class  prediction and the final prediction is determined by majority voting.\n",
        "\n",
        "* The Random forest algorithm is know for its effectiveness and versatility. it can handle a wide range of data and requires little configuration. it is often used as a black box model in businesses because it generates reasonable prediction without much tuning."
      ],
      "metadata": {
        "id": "71wlkp0317ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics_history[metrics_history['Model'] == 'Random Forest Regression']"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators' : 100, # trial.suggest_int('n_estimators', 100, 200),\n",
        "        'max_depth' : trial.suggest_int('max_depth', 15, 50),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 7),\n",
        "        'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 5, 10),\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    model = RandomForestRegressor(**params)\n",
        "    model.fit(X_train_s, y_train_s)\n",
        "    val_r2 = model.score(X_val_s, y_val_s)\n",
        "    print('Train r2 score: ', model.score(X_train_s, y_train_s))\n",
        "\n",
        "    return val_r2\n",
        "\n",
        "###--- Commenting out following code because HPO takes too long to run----###\n",
        "\n",
        "# study = optuna.create_study(direction= 'maximize')\n",
        "# study.optimize(objective, n_trials=25)\n",
        "\n",
        "# print('Best hyperparameters: ', study.best_trial.params)\n",
        "# print('Best r2 score: ', study.best_value)\n",
        "\n",
        "###-----End of HPO for Random Forest Regression---#"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| `n_estimators` | `max_depth` | `min_samples_split` | `min_samples_leaf` | Best R-squared Score |\n",
        "|-----------------|-------------|---------------------|--------------------|----------------------|\n",
        "| 100             | 43          | 5                    | 5                  | 0.8840932266273452 |\n",
        "| 50              | 24           | 7                    | 9                  | 0.8786926164161606 |\n",
        "| 25              | 24          | 7                    |\n",
        "\n",
        "\n",
        "These hyperparameters represent the best configuration found through the hyperparameter optimization process.The corresponding R-squared scores indicate the goodness of fit the model using these hyperparameter settings."
      ],
      "metadata": {
        "id": "Q4IzmS1y8LEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected hyperparameters from HPO\n",
        "params = {'n_estimators': 100, 'max_depth': 43, 'min_samples_split':5, 'min_samples_leaf': 5}\n",
        "\n",
        "# Use the best model to make predictions\n",
        "models = [('Random Forest Regression HPO', RandomForestRegressor(**params,random_state=42))]\n",
        "metrics_df,_ = Train_evaluate(models, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Save evaluation metrics to metrics_history\n",
        "metrics_history = metrics_history.append(metrics_df)\n",
        "metrics_history[(metrics_history['Model'] == 'Random Forest Regression') |\n",
        "                (metrics_history['Model'] == 'Random Forest Regression HPO')]\n"
      ],
      "metadata": {
        "id": "Vf8w3WvZNNBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The HPO technique has helped the model to reduce overfitting indicated by a lower R-squared score of 0.93 on the training set.The model reduce the values of evaluation metrics on the validation set which is 0.88"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - Gradient Boosting Regressor."
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting Machine (GBM)**\n",
        "* GBM is a machine learning technique that uses an ensemble of weak prediction models, typically decision tree, to create a strong learner.\n",
        "* GBM trains many models in gradual, additive & sequential manner and it is highly customizable to the particular needs of the application.\n",
        "* GBM involves three elements: a loss function to be optimized, a weak learner to make predictions & an additive model to add weak learner to minimize the loss function.\n",
        "* GBM can overfit a training dataset quickly, but regularization methods can improve its performance by reducing overfitting."
      ],
      "metadata": {
        "id": "PdhwcruqTc9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics_history[metrics_history['Model'] == 'Gradient Boosting Regression']"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The RMSE & MAE indicates that the model is making prediction with the average error of around 2300.\n",
        "* The R-squared & Adjusted R-squared values for both set are around 0.4, which means that the model is able to explain around 40% of the variance in the target variable."
      ],
      "metadata": {
        "id": "VrghZhVBV-f2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def objective(trial) :\n",
        "    params = {\n",
        "        'n_estimators' : 50, # trial.suggest_int('n_estimators', 50, 400),\n",
        "        'max_depth' : trial.suggest_int('max_depth', 7, 15),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
        "        'min_samples_split' : trial.suggest_int('min_sample_split', 2, 5),\n",
        "        'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1, 3),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
        "    }\n",
        "    model = GradientBoostingRegressor(**params)\n",
        "    model.fit(X_train_s, y_train_s)\n",
        "\n",
        "    y_pred = model.predict(X_val_s)\n",
        "    r2 = r2_score(y_val_s, y_pred)\n",
        "\n",
        "    return r2\n",
        "\n",
        "###---Commenting out following code because HPO takes too long to run -----###\n",
        "\n",
        "# study = optuna.create_study(direction = 'maximize')\n",
        "# study.optimize(objective, n_trials=10)\n",
        "\n",
        "# print('Best hyperparameters: ', study.best_trial.params)\n",
        "# Print('Best r2 score: ', study.best_value)\n",
        "\n",
        "###--- End of HPO for Gradient Boosting Regressor----###"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| `n_estimators` | `max_depth` | `learning_rate` | `min_samples_split` | `min_samples_leaf` | `subsample` | Best R-squared Score |\n",
        "|-----------------|-------------|-----------------|---------------------|--------------------|-------------|----------------------|\n",
        "| 100             | 12          | 0.12507294020594728 | 5                    | 2                  | 0.7857656224069701 | 0.9099075010286225 |\n",
        "| 50              | 13          | 0.06375016028751634 | 4                    | 3                  | 0.6921886023174675 | 0.7929251654166077 |\n",
        "| 25              | 11          | 0.13788691898021682 | 4                    | 2                  | 0.6801601804268422 | 0.7225825814634139 |\n",
        "\n",
        "\n",
        "* These hyperparameters represent the best configuration found through the hyperparameter optimization process. The corresponding R-Squared scores indicate the goodness of fit of the model using these hyperparameter settings."
      ],
      "metadata": {
        "id": "uF_MzK_YbL5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models\n",
        "params = {'n_estimators': 100,\n",
        "          'max_depth': 12,\n",
        "          'learning_rate': \t0.12507294020594728,\n",
        "          'min_samples_split': 5,\n",
        "          'min_samples_leaf': 2,\n",
        "          'subsample': 0.7857656224069701}\n",
        "\n",
        "          # use the best model to make predicitions.\n",
        "# Save evaluation metrics to metrics_history\n",
        "models = [('Gradient Boosting Regression HPO', GradientBoostingRegressor(**params, random_state=42))]\n",
        "\n",
        "metrics_df,_ = Train_evaluate(models, X_train_s, y_train_s, X_val_s, y_val_s)\n",
        "metrics_history = metrics_history.append(metrics_df)\n",
        "\n",
        "metrics_history[(metrics_history['Model'] == 'Gradient Boosting Regression') |\n",
        "                (metrics_history['Model'] == 'Gradient Boosting Regression HPO')]"
      ],
      "metadata": {
        "id": "2qLEYw3LcMfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The model has improved significantly after HPO indicatd by an increase in R-squared value and decrease in RMSE & MAE values.\n",
        "\n",
        "* The final R-squared value of 0.92 and RMSE of 4.97 indicated model is performing well on the validation set."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4- XGBRegressor."
      ],
      "metadata": {
        "id": "yIMZqYSIvbT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "r1Ar0yqCvqMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* XGBoos is a highly efficient and widely used machine learning algorithm based on gradient boosting.it combines weak predictive models, typically decision tree, to create a powerful ensemble model.\n",
        "* XGBoost incorporates regularization techniques to prevent overfitting and provide feature importance analysis. it handles missing values, supports parallel processing and allows for early stopping to find the optimal number of trees.\n",
        "* XGBoost is known for its speed, scalability and accuracy, making it suitable for various applications."
      ],
      "metadata": {
        "id": "J2qXsfwIwBTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics from XGBoost Regressor Model.\n",
        "metrics_history[metrics_history['Model'] == 'XGBoosting Regression']"
      ],
      "metadata": {
        "id": "3-VAvXk4-v2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The XGBoos Regression model demonstrates strong performance based on the evaluation metrics.\n",
        "* It has low value for RMSE & MAE, indicating accurate predictions with minimal errors.\n",
        "* The R-squared & Adjusted R-squared scores of 0.83 suggest that the model captures a significant portion (83%) of the target variables variance, indicating a good fit.\n",
        "* The model's performance is consistent on both the training and validation sets,highlighting its generalization ability."
      ],
      "metadata": {
        "id": "IBGiOmvs_n2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross-Validation & Hyperparameter Tuning."
      ],
      "metadata": {
        "id": "Pia1r066BMr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Enable GPU support\n",
        "xgb_config = {\n",
        "    'tree_method': 'gpu_hist',  # use GPU accelerated algorithm\n",
        "    'gpu_id' : 0  # Specify the GPU device index to use (e.g., 0 for the first GPU)\n",
        "}\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'max_depth': 20, #trial.suggest_int('max_depth', 3, 9),\n",
        "        'learning_rate': 0.11096130067213479, # trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 4, 10),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1),\n",
        "        'gamma': trial.suggest_uniform('gamma', 0.1, 0.4),\n",
        "    }\n",
        "    # Update the XGBoost parameters with GPU configuration\n",
        "    params.update(xgb_config)\n",
        "\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "    model.fit(X_train_s, y_train_s)\n",
        "\n",
        "    y_pred = model.predict(X_val_s)\n",
        "    r2 = r2_score(y_val_s, y_pred)\n",
        "\n",
        "    return r2\n",
        "\n",
        "###----- Commenting out following code because HPO takes too long to run----###\n",
        "\n",
        "# study = optuna.create_study(direction= 'maximize')\n",
        "# study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Print('Best hyperparameters:', study.best_trial.params)\n",
        "# print('Best r2 score: ', study.best_value)\n",
        "\n",
        "###---End of HPO for XGBoost Regressor-----###"
      ],
      "metadata": {
        "id": "-lTkJRirBX8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| `max_depth` | `min_child_weight` | `colsample_bytree` | `gamma` | Best R-squared Score |\n",
        "|-------------|--------------------|--------------------|---------|----------------------|\n",
        "| 20          | 9                  | 0.6007938923707836 | 0.1919642988536765 | 0.9254781023199998 |\n",
        "| 9           | 1                  | 0.7100240740796231 | 0.38772768285415593 | 0.8859478511092371 |\n",
        "\n",
        "\n",
        "* These hyperparameters represent the best configuration found through the hyperparameter optimization process. The corresponding R-squared score indicate the goodness of fit of the model using these hyperparameter settings."
      ],
      "metadata": {
        "id": "lOngub6OGCD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement ? Note down the improvement with updates Evaluation metric score charts."
      ],
      "metadata": {
        "id": "Nc0ei3qpG0F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected hyperparameters from HPO\n",
        "XGB_parameters = {\n",
        "     'max_depth':20,\n",
        "     'learning_rate': 0.11096130067213479,\n",
        "     'min_child_weight':9,\n",
        "     'colsample_bytree': 0.7100240740796231,\n",
        "    #  'tree_method' : gpu_hist, # use GPU accelerated algorithm\n",
        "}\n",
        "\n",
        "# Use the best model to make predictions\n",
        "models = [('XGBoost Regression HPO', XGBRegressor(**XGB_parameters, random_state= 42))]\n",
        "\n",
        "# Save Evaluation metrics to metrics_history\n",
        "metrics_df,XGB_model = Train_evaluate(models, X_train_s, y_train_s, X_val_s, y_val_s)\n",
        "metrics_history = metrics_history.append(metrics_df)\n",
        "\n",
        "metrics_history[(metrics_history['Model'] == 'XGBoosting Regression') |\n",
        "                (metrics_history['Model'] == 'XGBoost Regression HPO')]"
      ],
      "metadata": {
        "id": "ik1ACtk6HKgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After HPO the model has improved significantly with an train R-squared value of 0.98 and validation R-squared value of 0.93.\n",
        "\n",
        "* So far XGBoost has shown the best performance on the validation set with the minimum RSME & MAE values and highest R-squared."
      ],
      "metadata": {
        "id": "lM_Qdo2zTqSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Root Mean Squared Error (RMSE):\n",
        "   * RMSE measures the average deviation of predicted values from actual values.\n",
        "   * A lower RMSE suggests that the model's predictions are closer to the actual values, which is desirable for businesses relying on accurate predictions.\n",
        "\n",
        "2. Mean Absolute Error (MAE):\n",
        "   * MAE measures the average magnitude of error between predicted and actual values.\n",
        "   * MAE is more interpretable than RMSE since it represents the absolute difference between prediction and actual values.\n",
        "\n",
        "3. R-squared(R^2) Score:\n",
        "   * R-squared is a statistical measure that represents the proportion of the variance in the dependent variable (target)explained by the independent variables (features).\n",
        "   * R-Squared range from 0 to 1, where 1 indicates a perfect fit.\n",
        "   * Higher R-Squared values indicate better model performance in capturing the relationship between features and the target variable.\n",
        "\n",
        "4. Adjusted R-Squared:\n",
        "   * Adjusted R-Squared adjusts the R-squared value by considering the number of features and the sample size.\n",
        "   * It penalize models with too many features that may overfit the data.\n",
        "   * A higher adjusted R-squared value suggests better model performance in capturing the relevant information without overfitting.\n",
        "\n",
        "5. Mean Absolute Percentage Error(MAPE):\n",
        "   * MAPE measures the average percentage difference between predicted and actual values.\n",
        "   * it provide a relative measure of the prediction accuracy.\n",
        "   * Lower MAPE values indicate better accuracy, as the model's predictions are closed to the actual values.    "
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_history"
      ],
      "metadata": {
        "id": "uN8PcTLssWAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Names.\n",
        "models = [\n",
        "     \"Decision Tree Regression\",\n",
        "     \"Decision Tree Regression HPO\",\n",
        "     \"Random Forest Regression\",\n",
        "     \"Random Forest Regression HPO\",\n",
        "     \"Gradient Boosting Regression\",\n",
        "     \"Gradient Boosting Regression HPO\",\n",
        "     \"XGBoosting Regression\",\n",
        "     \"XGBoost Regression HPO\"\n",
        "]\n",
        "\n",
        "# R squared values\n",
        "eval_r_squared = [0.81, 0.85, 0.88, 0.88, 0.40, 0.92, 0.83, 0.93]\n",
        "\n",
        "# Runtime information.\n",
        "runtime = [1.13, 0.47, 74.15, 40.15, 1.92, 6.83, 1.67, 37.55]\n",
        "\n",
        "# Create a data frame from the above data.\n",
        "data = pd.DataFrame({\"Model\": models, \"Validation R-squared\":eval_r_squared, \"Runtime\":runtime })\n",
        "\n",
        "# Sort the dataframe by R-squared value.\n",
        "data.sort_values(by= \"Validation R-squared\", inplace=True, ascending=False)\n",
        "\n",
        "# Set the color palette.\n",
        "colors= [\"steelblue\" if \"HPO\" not in model else \"salmon\" for model in data['Model']]\n",
        "\n",
        "# Create a 1x2 subplot grid (1 row, 2 columns)\n",
        "gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Create a horizontal bar plot using seaborn in the first sublot.\n",
        "ax1 = plt.subplot(gs[0])\n",
        "sns.barplot(x= \"Validation R-squared\", y=\"Model\", data=data, palette=colors, ax=ax1)\n",
        "ax1.set_xlabel(\"Validation R-squared\")\n",
        "ax1.set_ylabel(\"Model\")\n",
        "ax1.set_title(\"Comparison of validation R-squared for different Models\")\n",
        "\n",
        "# Add annotations for the corresponding R-squared values.\n",
        "for i, v in enumerate(data[\"Validation R-squared\"]):\n",
        "    ax1.text(v + 0.01, i, str(v), color= 'black', va='center')\n",
        "\n",
        "# Create the line plot in the second subplot.\n",
        "ax2 = plt.subplot(gs[1])\n",
        "sns.lineplot(x=\"Runtime\", y=\"Model\", data=data, sort=False, ax=ax2)\n",
        "ax2.set_xlabel(\"Runtime (seconds)\")\n",
        "ax2.set_ylabel(\"\")\n",
        "ax2.set_title(\"Model Runtime Comparison\")\n",
        "ax2.yaxis.tick_right()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eRotKE7YsqxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observing the chart, the XGBoost Regression HPO(Hyperparameter Optimized) model has the highest validation R-squared 0.93, indicating the best predictive power.However, it also a relatively high runtime at 37.55 seconds.\n",
        "\n",
        "On the other hand, the Gradient Boosting Regression HPO model has a slightly lower validation R-squared(0.92), but it still fairly high and also has significantly shorter runtime(6.83 seconds).\n",
        "\n",
        "If runtime is a critical factor (say, in a productive enviroment where prediction need to be made quickly), we could pick Gradient Boosting Regression HPO model as it strikes good balance between predictive power & efficiency.\n",
        "\n",
        "However, Since we are seeking the highest possible predictive power the XGBoost Regression HPO would be the better choice."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importance = XGB_model[0][1].feature_importances_\n",
        "\n",
        "# Create a Dataframe with feature names and importance scores.\n",
        "feature_importances_df = pd.DataFrame({'Feature': X_train_s.columns, 'Importance': importance})\n",
        "\n",
        "# Sort the Dataframe by importance socres in descending order.\n",
        "feature_importances_df = feature_importances_df.sort_values(by= 'Importance', ascending=False)\n",
        "\n",
        "# plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.barh(feature_importances_df['Feature'], feature_importances_df['Importance'] )\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.title('XGBoost Feature Importance')\n",
        "\n",
        "# Adding corresponding values to the bars\n",
        "for bar in bars:\n",
        "  plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2,\n",
        "           '{0:.3f}'.format(bar.get_width()),\n",
        "           va='center', ha='left')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IpL-RB3ZY5pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature importance** gives an idea of how much each feature contributes to the model's prediction.The feature importance scores are calculated based on the reduction in the criterion used to select split points, like Gini or entropy.\n",
        "\n",
        "  * In this case the most important feature for predictting the target variable is Promo, with an importance score of about 0.441.\n",
        "  * The second and third most important features are StoreType_b and StateHoliday_0 with importance score of 0.114 and 0.038 respectively."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "pickle.dump(metrics_history, open('metrics_history2.pkl','wb'))\n",
        "pickle.dump(XGB_model, open('XGB_model2.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Commenting out since this only needed to be done once.\n",
        "# !cp metrics_history2.pkl '/content/drive/MyDrive/Almabetter/Almabetter Projects/Hotel Booking'\n",
        "# ! cp XGB_model2.pkl '/content/drive/MyDrive/Almabetter/Almabetter Projects/Hotel Booking"
      ],
      "metadata": {
        "id": "ECBEXjGGfcQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Commenting out since this only needed to be done once\n",
        "# Load the File and predict unseen data.\n",
        "# import pickle\n",
        "# metrics_history = pickle.load(open('/content/drive/MyDrive/Almabetter/Almabetter Project/Hotel Booking/metrics_history.pkl', 'rb'))\n",
        "# XGB_model = pickle.load(open('/content/drive/MyDrive/Almabetter/Almabetter Projects/Hotel Booking/XGB_model.pkl','rb'))"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction = XGB_model[0][1].predict(X_test_s)\n",
        "r2_test = r2_score(y_test_s, test_prediction)\n",
        "r2_test\n",
        "rmse = np.sqrt(mean_squared_error(y_test_s, test_prediction))\n",
        "mae = mean_absolute_error(y_test_s, test_prediction)\n",
        "print('Test r2 score: ', r2_test)\n",
        "print('Test rmse: ',rmse)\n",
        "print('Test mae: ', mae)"
      ],
      "metadata": {
        "id": "76fDZFW8hyjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([y_test_s.min(), y_test_s.max()], [y_test_s.min(), y_test_s.max()], 'k--', lw=4)\n",
        "plt.scatter(y_test_s, test_prediction)"
      ],
      "metadata": {
        "id": "S_zEaGaljLZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Merg the predicted and actual values with the corresponding dates.\n",
        "merged_data = X_test_s.join(df_m2['Date'])\n",
        "merged_data['Actual'] = y_test_s\n",
        "merged_data['Predicted'] = test_prediction\n",
        "\n",
        "#  Calculate the daily mean values.\n",
        "daily_mean_actual = merged_data.groupby('Date')['Actual'].mean()\n",
        "daily_mean_predicted = merged_data.groupby('Date')['Predicted'].mean()\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the daily mean values.\n",
        "plt.plot(daily_mean_actual.index, daily_mean_actual, label='Actual')\n",
        "plt.plot(daily_mean_predicted.index, daily_mean_predicted, label='Predicted')\n",
        "\n",
        "# Rotate the x-axis labels for better readability\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Prediction on Unseen Data \\n 6 - Week Forecast of Average Daily Sales VS Actual Average Daily Sales')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "M47JBzgAj9tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the key findings from EDA.\n",
        "\n",
        "  1. Seasonal Variation: The business experiences a seasonal variation in sales, with higher sales during the holiday season. To mitigate negative growth during non-peak seasons, strategies should be explored to stimulate demand and attract customer during off-peak periods.\n",
        "\n",
        "  2. Competition: The presence of competiton impacts sales, with areas having higher competition generating higher sales.Businesses should consider competiton when planning store locations and marketing strategies.\n",
        "\n",
        "  3. Promotions: Promotional periods generally lead to higher sales.Business should carefully plan and optimize promotional activities to maximize sales impact.\n",
        "\n",
        "  4. Store Type and Assortment: Store type 'b' assortment type 'c' consistently performed better in terms of sales. Businesses could focus on leveraging the strengths of these types while considering strategies for improving the performance of other types.\n",
        "\n",
        "  5. Sales Volume: The overall sales volume showed a positive trend, indicating a growing number of sales transactions. Businesses should monitor and maintain this growth to sustain revenue generation.\n",
        "\n",
        "  In the next part, We trained and evaluated several regression models,including Decision Tree Regression, Random forest Regression,\n",
        "  Gradient Boosting Regression and XGBoost Regression. We also performed hyperparameter optimization for the above models.\n",
        "\n",
        "These are the observation after training and testing above models.\n",
        "\n",
        "  1. In this initial training of models Random forest regressor obtained the highest R-squared value of 0.88, on the validation set.\n",
        "  2. After tuning the hyperparameters the XGBoos model achieved the highest R-squared value of 0.93 on validation set, indicating that it explains a siginificant portion of the variance in  the target variable.\n",
        "  3. XGBoost model has also obtained a R-squared value of 0.88 on the test data.\n",
        "  4. So according to our observations it can be concluded that XGBoost Regression is the optimal model for predicting Rossmann's sales data."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}